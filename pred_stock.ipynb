{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CODERZZZ123/StockPred_NewsHeadlines/blob/master/pred_stock.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9xkIi4fT0sK",
        "outputId": "490cad32-e328-4d52-c834-5e2e9f72155d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk.corpus\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import matplotlib.pyplot as plt\n",
        "# !pip install sklearn\n",
        "import random\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "# !pip install textattack\n",
        "# !pip install --upgrade gensim --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hIHZTthYA0tA"
      },
      "outputs": [],
      "source": [
        "# import gensim\n",
        "\n",
        "# !pip install transformers --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aSAHOHjzBHzN"
      },
      "outputs": [],
      "source": [
        "# import transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IMYmDEBVBl6R"
      },
      "outputs": [],
      "source": [
        "# !pip install sacremoses --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NsQEyXUxBq2U"
      },
      "outputs": [],
      "source": [
        "# import sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZzENF1KrBvIG"
      },
      "outputs": [],
      "source": [
        "# !pip install nlpaug --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wByHAA0DB0Ch"
      },
      "outputs": [],
      "source": [
        "# import nlpaug.augmenter.char as na\n",
        "# import nlpaug.augmenter.word as naw\n",
        "# import nlpaug.augmenter.sentence as nas\n",
        "# import nlpaug.flow as nafc\n",
        "# from nlpaug.util import Action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "atzWm9jsCLEk"
      },
      "outputs": [],
      "source": [
        "# from nlpaug.util.file.download import DownloadUtil\n",
        "# DownloadUtil.download_word2vec(dest_dir = \".\")\n",
        "# DownloadUtil.download_fasttext(dest_dir = \".\" , model_name = 'crawl-300d-2M')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rN7FEZ0ZuqFO"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# import requests\n",
        "# import time\n",
        "\n",
        "# while True:\n",
        "#     try:\n",
        "#         requests.get('https://www.google.com')\n",
        "#         print(\"Kept alive.\")\n",
        "#     except:\n",
        "#         print(\"Failed to keep alive.\")\n",
        "#     time.sleep(600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RNqcUOuIT0sM"
      },
      "outputs": [],
      "source": [
        "from sklearn import  set_config\n",
        "set_config(display = 'diagram')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1s_eLsA1T0sN"
      },
      "outputs": [],
      "source": [
        "df_NewsDjia = pd.read_csv('News_DJIA.csv', delimiter=',')\n",
        "# df_DjiaStock = pd.read_csv('DJIA_t.csv',delimiter=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Im-8hHxS1heY"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Euw0z2hIPiPN"
      },
      "outputs": [],
      "source": [
        "# df_DjiaStock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WIQ6Fn1iT0sN"
      },
      "outputs": [],
      "source": [
        "for i in df_NewsDjia.columns :\n",
        "    df_NewsDjia[i] = df_NewsDjia[i].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "76jhl5TfT0sN"
      },
      "outputs": [],
      "source": [
        "# df_NewsDjia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "AdF2Yv7oT0sN"
      },
      "outputs": [],
      "source": [
        "num_columns = df_NewsDjia.columns.shape[0]\n",
        "col_names = df_NewsDjia.columns.values.tolist()\n",
        "col_names = col_names[1:]\n",
        "df_NewsDjia.loc[:, 'merged'] = \" \"\n",
        "for each_col_ind in range(num_columns-1):\n",
        "    df_NewsDjia.loc[:, 'merged'] =  df_NewsDjia.loc[:, 'merged'] + \" \" + df_NewsDjia[col_names[each_col_ind]] + \" \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "QpJYzG1RT0sO"
      },
      "outputs": [],
      "source": [
        "df_finalised = df_NewsDjia[['merged']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "aTSmdMp5T0sO"
      },
      "outputs": [],
      "source": [
        "date_numric_stopwords = \"THOUSAND MILLION BILLION TRILLION HUNDRED ANNUAL ANNUALLY ANNUM YEAR YEARLY QUARTER QUARTERLY QTR MONTH MONTHLY WEEK WEEKLY DAY DAILY JANUARY FEBRUARY MARCH APRIL MAY JUNE JULY AUGUST SEPTEMBER OCTOBER NOVEMBER DECEMBER JAN FEB MAR APR MAY JUN JUL AUG SEP SEPT OCT NOV DEC MONDAY TUESDAY WEDNESDAY THURSDAY FRIDAY SATURDAY SUNDAY TWO THREE FOUR FIVE SIX SEVEN EIGHT NINE TEN ELEVEN TWELVE THIRTEEN FOURTEEN FIFTEEN SIXTEEN SEVENTEEN EIGHTEEN NINETEEN TWENTY THIRTY FORTY FIFTY SIXTY SEVENTY EIGHTY NINETY FIRST SECOND THIRD FOURTH FIFTH SIXTH SEVENTH EIGHTH NINTH TENTH II III IV V VI VII VIII IX X XI XII XIII XIV XV XVI XVII XVIII XIX XX\"\n",
        "date_numric_stopwords = date_numric_stopwords.lower()\n",
        "date_numric_stopwords = date_numric_stopwords.split(\" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "m5cRP3mjT0sO"
      },
      "outputs": [],
      "source": [
        "\n",
        "def remove_unecssary_word(sent):\n",
        "    sent = sent.replace(\"b'\",'')\n",
        "    sent = sent.replace('b\"','')\n",
        "    sent = sent.lower()\n",
        "    sent = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", sent)\n",
        "    stop = stopwords.words('english')\n",
        "    stop.extend(date_numric_stopwords)\n",
        "    sent = \" \".join([word for word in sent.split() if word not in (stop)])\n",
        "    sent = \" \".join([WordNetLemmatizer().lemmatize(word) for word in sent.split() ])\n",
        "    return sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnZxsRH_T0sO",
        "outputId": "f8b7148b-6fe0-4153-ccdf-d6ef5d3f0956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-6847205c14ea>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_finalised['merged'] = df_finalised['merged'].apply(remove_unecssary_word)\n"
          ]
        }
      ],
      "source": [
        "df_finalised['merged'] = df_finalised['merged'].apply(remove_unecssary_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "Wx0BhxO9T0sO",
        "outputId": "6e61b6bb-a2a7-4372-eac1-932fdb98900b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0 georgia down russian warplane country move brink war breaking musharraf impeached russia today column troop roll south ossetia footage fighting youtube russian tank moving towards capital south ossetia reportedly completely destroyed georgian artillery fire afghan child raped impunity un official say sick old raped nothing 150 russian tank entered south ossetia whilst georgia shoot russian jet breaking georgia invades south ossetia russia warned would intervene so side enemy combatent trial nothing sham salim haman sentenced 5 12 year kept longer anyway feel like georgian troop retreat osettain capital presumably leaving several people killed video u prep georgia war russia rice give green light israel attack iran say u veto israeli military ops announcingclass action lawsuit behalf american public fbi sorussia georgia war nyts top story opening ceremony olympics fucking disgrace yet proof decline journalism china tell bush stay country affair world war start today georgia invades south ossetia russia get involved nato absorb georgia unleash full scale war alqaeda face islamist backlash condoleezza rice u would act prevent israeli strike iran israeli defense minister ehud barak israel prepared uncompromising victory case military hostility busy european union approved new sanction iran protest nuclear programme georgia withdraw 1000 soldier iraq help fight russian force georgia breakaway region south ossetia pentagon think attacking iran bad idea u news amp world report caucasus crisis georgia invades south ossetia indian shoe manufactory series like work visitor suffering mental illness banned olympics help mexico kidnapping surge'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "df_finalised.iloc[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "A5SR3rl8Uw2y"
      },
      "outputs": [],
      "source": [
        "# from textattack.augmentation import EmbeddingAugmenter\n",
        "# embed_aug = EmbeddingAugmenter()\n",
        "# embed_aug.augment(df_finalised.iloc[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "7SSR3l6UEIGr"
      },
      "outputs": [],
      "source": [
        "# from textattack.augmentation import WordNetAugmenter\n",
        "# wordnet_aug = WordNetAugmenter()\n",
        "# wordnet_aug.augment(df_finalised.iloc[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "K7x4r45pE-6C"
      },
      "outputs": [],
      "source": [
        "# from textattack.augmentation import EasyDataAugmenter\n",
        "# eda_aug = EasyDataAugmenter()\n",
        "# eda_aug.augment(df_finalised.iloc[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Wv9DIdYsT0sP"
      },
      "outputs": [],
      "source": [
        "df_finalised = pd.concat([df_finalised,df_NewsDjia['Label']],axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "M42KsKBjT0sP"
      },
      "outputs": [],
      "source": [
        "# Positive_word = \"\"\n",
        "# with open('Positive_words.txt', 'r') as file:\n",
        "#     Positive_word = file.read().replace('\\n', '')\n",
        "# Positive_word = \" \".join([WordNetLemmatizer().lemmatize(word) for word in Positive_word.split(',') ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "oi832ifcT0sP"
      },
      "outputs": [],
      "source": [
        "# Negative_word = \"\"\n",
        "# with open('Negative_words.txt','r') as file:\n",
        "#     Negative_word = file.read().replace('\\n','')\n",
        "# Negative_word = \" \".join([WordNetLemmatizer().lemmatize(word) for word in Negative_word.split(',') ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "cen4cv_yT0sP"
      },
      "outputs": [],
      "source": [
        "# Positive_word = Positive_word.split(',')\n",
        "# Negative_word = Negative_word.split(',')\n",
        "# Negative_word = Negative_word[0].split()\n",
        "# Positive_word = Positive_word[0].split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "vQkp4GoWT0sP"
      },
      "outputs": [],
      "source": [
        "# set_post = set(Positive_word)\n",
        "# set_neg = set(Negative_word)\n",
        "# def Count(obj):\n",
        "#     count_post = 1\n",
        "#     count_neg = 1\n",
        "#     obj = obj.split()\n",
        "#     for i in obj:\n",
        "#         if i in set_post:\n",
        "#             count_post = count_post + 1\n",
        "#         elif i in set_neg:\n",
        "#             count_neg = count_neg + 1\n",
        "#     return count_post/count_neg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "dOZFtn3KT0sP"
      },
      "outputs": [],
      "source": [
        "# df_finalised['ratio_pos_neg'] = df_finalised['merged'].apply(Count)\n",
        "# ratio_column = df_finalised['ratio_pos_neg']\n",
        "# ratio_column = np.array(ratio_column).reshape((-1,1))\n",
        "X_data = df_finalised['merged']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "9NYhdmewZQ5w"
      },
      "outputs": [],
      "source": [
        "Y_data = df_finalised['Label']\n",
        "Y_data = Y_data.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "ASqt7TciSdK6",
        "outputId": "54321419-777d-4005-e911-f2bc1aa80000"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0 georgia down russian warplane country move brink war breaking musharraf impeached russia today column troop roll south ossetia footage fighting youtube russian tank moving towards capital south ossetia reportedly completely destroyed georgian artillery fire afghan child raped impunity un official say sick old raped nothing 150 russian tank entered south ossetia whilst georgia shoot russian jet breaking georgia invades south ossetia russia warned would intervene so side enemy combatent trial nothing sham salim haman sentenced 5 12 year kept longer anyway feel like georgian troop retreat osettain capital presumably leaving several people killed video u prep georgia war russia rice give green light israel attack iran say u veto israeli military ops announcingclass action lawsuit behalf american public fbi sorussia georgia war nyts top story opening ceremony olympics fucking disgrace yet proof decline journalism china tell bush stay country affair world war start today georgia invades south ossetia russia get involved nato absorb georgia unleash full scale war alqaeda face islamist backlash condoleezza rice u would act prevent israeli strike iran israeli defense minister ehud barak israel prepared uncompromising victory case military hostility busy european union approved new sanction iran protest nuclear programme georgia withdraw 1000 soldier iraq help fight russian force georgia breakaway region south ossetia pentagon think attacking iran bad idea u news amp world report caucasus crisis georgia invades south ossetia indian shoe manufactory series like work visitor suffering mental illness banned olympics help mexico kidnapping surge'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "X_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "AWJM39iIGonj"
      },
      "outputs": [],
      "source": [
        "# from textattack.augmentation import EasyDataAugmenter\n",
        "# eda_aug = EasyDataAugmenter()\n",
        "# data_X = []\n",
        "# label = []\n",
        "# # open('/content/drive/MyDrive/Colab Notebooks/Pred_stock_newheadline/data.txt', 'w').close()\n",
        "# # open('/content/drive/MyDrive/Colab Notebooks/Pred_stock_newheadline/label.txt','w').close()\n",
        "# with open('/content/drive/MyDrive/Colab Notebooks/Pred_stock_newheadline/data.txt', 'a') as f1 , open('/content/drive/MyDrive/Colab Notebooks/Pred_stock_newheadline/label.txt','a') as f2:\n",
        "#   for i in range(0,X_data.shape[0]):\n",
        "#     data_X.append(X_data[i])\n",
        "#     label.append(Y_data[i])\n",
        "#     f1.write(X_data[i])\n",
        "#     f1.write(\",\")\n",
        "#     y = str(Y_data[i])\n",
        "#     f2.write(y)\n",
        "#     f2.write(\",\")\n",
        "#     if i%10 == 0:\n",
        "#       data = eda_aug.augment(X_data[i])\n",
        "#       for d in data:\n",
        "#         data_X.append(d)\n",
        "#         label.append(Y_data[i])\n",
        "#         f1.write(d)\n",
        "#         f1.write(\",\")\n",
        "#         f2.write(y)\n",
        "#         f2.write(\",\")\n",
        "\n",
        "#     print(i)\n",
        "# open('/content/drive/MyDrive/Colab Notebooks/Pred_stock_newheadline/train_data.txt', 'w').close()\n",
        "\n",
        "# Note test sample isbeing already saved aside so no need to run it again\n",
        "# open('/content/drive/MyDrive/Colab Notebooks/Pred_stock_newheadline/test_data.txt','w').close()\n",
        "\n",
        "# for i in range(0,X_data.shape[0]//10+1):\n",
        "#   if i >= X_data.shape[0]:\n",
        "#     break\n",
        "\n",
        "#   random_numbers = random.sample(range(0, 10), 5)\n",
        "#   for k,l in enumerate(random_numbers): random_numbers[k] += i*10\n",
        "#   random_numbers.sort()\n",
        "#   # print(random_numbers)\n",
        "#   for j in range(0,10):\n",
        "#     if j + i*10 >= X_data.shape[0] :\n",
        "#       break\n",
        "#     if j + i*10 in random_numbers:\n",
        "#       with open('/content/drive/MyDrive/Colab Notebooks/Pred_stock_newheadline/train_data.txt', 'a') as f1 :\n",
        "        # code for te text augementation on the jth index\n",
        "        # f1.write(\"\")\n",
        "        # t = 0\n",
        "\n",
        "    # else:\n",
        "    #   with open('/content/drive/MyDrive/Colab Notebooks/Pred_stock_newheadline/test_data.txt', 'a') as f2:\n",
        "    #     # code for the text augmentation on the jth index\n",
        "    #     f2.write(X_data[j + i*10])\n",
        "    #     f2.write(\",\")\n",
        "    # print(j + i*10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pm29KgKnZULw",
        "outputId": "db4c7063-a9da-4476-cfa6-d39ca240f412"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, ..., 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "Y_data = np.array(Y_data)\n",
        "Y_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Ji_NATWSH5Lc"
      },
      "outputs": [],
      "source": [
        "# X_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "drGIIgbRy3un"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "9EUKXPVmWLIL"
      },
      "outputs": [],
      "source": [
        "# data_X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "kETBXbGpT0sP"
      },
      "outputs": [],
      "source": [
        "data_augmented = np.genfromtxt('/content/drive/MyDrive/Colab Notebooks/Pred_stock_newheadline/data.txt',dtype = str , delimiter=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSSZigVrT0sP",
        "outputId": "d9d398bf-81e0-4ec6-c26b-eb30cc9709f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2864,)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "data_augmented.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArtMidVuNwaZ",
        "outputId": "9767de9c-1134-4797-dde4-d5ce5599af5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2788\n",
            "2864\n"
          ]
        }
      ],
      "source": [
        "count = 0\n",
        "count2 = 0\n",
        "for i in data_augmented:\n",
        "    if len(i) != 0:\n",
        "        if i[0] =='0' or i[0] == '1':\n",
        "            if i[1] == \" \":\n",
        "\n",
        "              count = count + 1\n",
        "    count2 = count2 + 1\n",
        "print(count)\n",
        "print(count2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "7pQPHfpfN20C"
      },
      "outputs": [],
      "source": [
        "index = 0\n",
        "for i in data_augmented:\n",
        "  if len(i) != 0 :\n",
        "    if i[0] != '1' and i[0] != '0' :\n",
        "        data_augmented[index] = data_augmented[index-1][0] + \" \" + i\n",
        "    index += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRtyCRn3CFL8",
        "outputId": "1b9355d1-c4f4-4de4-fdd3-ce16045cbf72"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2864,)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "data_augmented.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tbYTHZwQDVP",
        "outputId": "73ee8383-028c-4be2-e71d-2f2e1dfa647d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2864,)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "data_augmented.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "XF9KVQNz2dOp"
      },
      "outputs": [],
      "source": [
        "data_augmented = data_augmented[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "-VT5orgXRvTr"
      },
      "outputs": [],
      "source": [
        "label_updated = data_augmented.astype('<U1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4sSo8s9TLGl",
        "outputId": "c9c0bbc8-47bd-4b08-e82a-df96766bbb73"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['0', '0', '0', ..., '1', '1', '0'], dtype='<U1')"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "label_updated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f36xhhHWXVNa",
        "outputId": "afc2989b-ce46-4a4c-eaea-31574c139110"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-48-39bc489429a0>:1: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  label_updated = label_updated.astype(np.float)\n"
          ]
        }
      ],
      "source": [
        "label_updated = label_updated.astype(np.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "FHkqIvD9S0wu"
      },
      "outputs": [],
      "source": [
        "data_augmented = list(data_augmented)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "M4rxs4nJSZFx"
      },
      "outputs": [],
      "source": [
        "data_augmented = [i[2:] for i in data_augmented]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "9WIy5x2oSgTy"
      },
      "outputs": [],
      "source": [
        "# data_augmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxVp8lhxT0sP",
        "outputId": "44dcf117-42fd-49f7-ed95-661eb109699b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1989,)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "X_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "meXgTNQtT0sQ"
      },
      "outputs": [],
      "source": [
        "# x_train , x_test , y_train, y_test = train_test_split(data_augmented,label_updated , test_size = 0.2 , random_state = 10)\n",
        "x_train , x_test , y_train , y_test = data_augmented[:2290] , data_augmented[2290:] , label_updated[:2290] , label_updated[2290:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "S2rkrR9OT0sQ"
      },
      "outputs": [],
      "source": [
        "tf_idf = TfidfVectorizer(min_df=10  )\n",
        "x_train_tf = tf_idf.fit_transform(x_train)\n",
        "x_test_tf = tf_idf.transform(x_test)\n",
        "x_train = x_train_tf.toarray()\n",
        "x_test = x_test_tf.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "cpJ8wLIdT0sQ"
      },
      "outputs": [],
      "source": [
        "# mean_not_zero = x_train[x_train != 0].mean()\n",
        "# x_train[x_train == 0 ] = x_train[x_train == 0] +  mean_not_zero\n",
        "# x_test[x_test == 0 ] = x_test[x_test == 0] + mean_not_zero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "nMAN-8NhT0sQ"
      },
      "outputs": [],
      "source": [
        "# ct_vect = CountVectorizer(ngram_range=(1,2) , min_df = 5)\n",
        "# x_train = ct_vect.fit_transform(x_train).toarray()\n",
        "# x_test = ct_vect.transform(x_test).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "WITB1UvPLDbG"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "CNrl6WtzT0sQ"
      },
      "outputs": [],
      "source": [
        "# scaler = StandardScaler()\n",
        "# scaler.fit(x_train)\n",
        "# x_train = scaler.transform(x_train)\n",
        "# x_test = scaler.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "9VWjlHJyT0sQ"
      },
      "outputs": [],
      "source": [
        "# pca = PCA()\n",
        "# pca.fit(x_train)\n",
        "# cumsum = np.cumsum(pca.explained_variance_ratio_)*100\n",
        "# d = [n for n in range(len(cumsum))]\n",
        "# plt.figure(figsize=(10, 10))\n",
        "# plt.plot(d,cumsum, color = 'red',label='cumulative explained variance')\n",
        "# plt.title('Cumulative Explained Variance as a Function of the Number of Components')\n",
        "# plt.ylabel('Cumulative Explained variance')\n",
        "# plt.xlabel('Principal components')\n",
        "# plt.axhline(y = 90, color='k', linestyle='--', label = '95% Explained Variance')\n",
        "# plt.legend(loc='best')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "Vkt-lDv5T0sQ"
      },
      "outputs": [],
      "source": [
        "# # VARIANCE EXPLAINED 90%\n",
        "# pca = PCA(0.90)\n",
        "# pca.fit(x_train)\n",
        "# x_train = pca.transform(x_train)\n",
        "# x_test = pca.transform(x_test)\n",
        "# pca.explained_variance_ratio_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "0bwJUnIIT0sQ"
      },
      "outputs": [],
      "source": [
        "# K = [int(i) for i in range(x_train.shape[0]//3)]\n",
        "# param_grid = {\n",
        "#     'n_neighbors' : K\n",
        "# }\n",
        "# knn = KNeighborsClassifier()\n",
        "# grid_search = GridSearchCV(estimator=knn,scoring='accuracy',param_grid=param_grid,cv = 10,n_jobs = -1,verbose = 2)\n",
        "# grid_search.fit(x_train,y_train)\n",
        "# grid_search.best_estimator_\n",
        "# knn = KNeighborsClassifier(n_neighbors=1)\n",
        "# knn.fit(x_train,y_train)\n",
        "# y_pred = knn.predict(x_test)\n",
        "# print(accuracy_score(y_pred,y_test))\n",
        "# print(classification_report(y_pred,y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "qIVzORqM4cJi",
        "outputId": "eddbd323-f620-4276-ac7e-99911e05413c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 28 candidates, totalling 140 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 28 is smaller than n_iter=100. Running 28 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-d90f1a90dbd6>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mLoRG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mrandom_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoRG\u001b[0m  \u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_distributions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m  \u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m  \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_for_distance_based\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mbest_param_LoRG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# best_param_LoRG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1769\u001b[0m             ParameterSampler(\n\u001b[1;32m   1770\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1950\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1705\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1706\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1708\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# scaler = StandardScaler()\n",
        "# scaler.fit(x_train)\n",
        "# x_train_for_distance_based = scaler.transform(x_train)\n",
        "# x_test_for_distance_based = scaler.transform(x_test)\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# penalty = ['l1','l2']\n",
        "# # max_iter= [1000]\n",
        "# C= [ 0.01,0.05 , 0.07 , 0.3,0.1,0.5,1,5,10,20,30,40,70,100]\n",
        "# solver = ['lbfgs']\n",
        "# param_grid = {\n",
        "#     'penalty' : penalty,\n",
        "#     # 'max_iter' : max_iter,\n",
        "#     'solver' : solver,\n",
        "#     'C' : C\n",
        "# }\n",
        "# LoRG = LogisticRegression(max_iter = 1000)\n",
        "# random_search = RandomizedSearchCV(estimator = LoRG  , n_iter=100 , scoring = 'accuracy', param_distributions = param_grid , cv  = 5  , n_jobs = -1, verbose = 2  )\n",
        "# random_search.fit(x_train_for_distance_based,y_train)\n",
        "# best_param_LoRG = random_search.best_estimator_\n",
        "# # best_param_LoRG\n",
        "# print(accuracy_score(y_test  , best_param_LoRG.predict(x_test_for_distance_based)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_m1 = LogisticRegression(penalty = 'l2' , solver = 'lbfgs'  , max_iter = 5000  , intercept_scaling=10)\n",
        "log_m2 = LogisticRegression(penalty = 'l2' , solver = 'liblinear'  , max_iter = 5000  , intercept_scaling=10)\n",
        "log_m3 = LogisticRegression(penalty = 'l2' , solver = 'newton-cg'  , max_iter = 5000  , intercept_scaling = 10)\n",
        "log_m4 = LogisticRegression(penalty = 'l2' , solver = 'newton-cholesky'  , max_iter = 5000 , intercept_scaling = 10 )\n",
        "log_m5 = LogisticRegression(penalty = 'l1' , solver = 'liblinear'  , max_iter = 5000  , intercept_scaling = 10)"
      ],
      "metadata": {
        "id": "maWXAYGkJb4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "# log_m1 = LogisticRegression(penalty = 'l2' , solver = 'lbfgs'  , max_iter = 1000 )\n",
        "# log_m1 = LogisticRegression(penalty = 'l2' , solver = 'lbfgs'  , max_iter = 1000 )\n",
        "estimators = [('log_m1' , log_m1) , ('log_m2' , log_m2) , ('log_m3' , log_m3) , ('log_m4' , log_m4) , ('log_m5', log_m5)]\n",
        "VCL = VotingClassifier(estimators = estimators)\n",
        "voting = ['hard' , 'soft']\n",
        "param_grid  = {\n",
        "    'voting' : voting\n",
        "}\n",
        "grid_search = GridSearchCV(estimator = VCL , scoring = 'accuracy' , param_grid = param_grid , cv = 5 , n_jobs = -1 , verbose = 2)\n",
        "grid_search.fit(x_train,y_train)\n",
        "best_param_VCl = grid_search.best_estimator_\n",
        "print(accuracy_score( best_param_VCl.predict(x_test), y_test ))"
      ],
      "metadata": {
        "id": "-yfWF56dz4rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_m1.fit(x_train,y_train)\n",
        "log_m2.fit(x_train,y_train)\n",
        "log_m3.fit(x_train,y_train)\n",
        "log_m4.fit(x_train,y_train)\n",
        "log_m5.fit(x_train,y_train)\n",
        "print(accuracy_score(y_test,log_m1.predict(x_test)))\n",
        "print(accuracy_score(y_test,log_m2.predict(x_test)))\n",
        "print(accuracy_score(y_test,log_m3.predict(x_test)))\n",
        "print(accuracy_score(y_test,log_m4.predict(x_test)))\n",
        "print(accuracy_score(y_test,log_m5.predict(x_test)))"
      ],
      "metadata": {
        "id": "uRhuX-DN5vJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SGD\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "loss = ['hinge', 'log_loss'  , 'perceptron']\n",
        "alpha = [0.000001, 0.00001 , 0.0001 , 0.001]\n",
        "max_iter = [1000 , 2000]\n",
        "tol = [0.0001 , 0.001 , 0.01 , 0.1]\n",
        "eta0= [0.001, 0.01,0.1]\n",
        "learning_rate = ['optimal' , 'adaptive']\n",
        "param_grid = {\n",
        "      'loss' : loss ,\n",
        "      'alpha' : alpha,\n",
        "      'max_iter' :max_iter,\n",
        "      'tol' : tol,\n",
        "      'learning_rate' : learning_rate,\n",
        "      'eta0' : eta0\n",
        "}\n",
        "sgd = SGDClassifier(early_stopping=True)\n",
        "random_search = RandomizedSearchCV(estimator = sgd ,n_iter=100 , scoring = 'accuracy', param_distributions = param_grid , cv  = 4  , n_jobs = -1, verbose = 2 , error_score='raise' )\n",
        "random_search.fit(x_train,y_train)\n",
        "best_estimator_sgd = random_search.best_estimator_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xteNW1JsHvLx",
        "outputId": "c76953fc-7125-4f36-d3ff-8dc84ae459b2"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 4 folds for each of 100 candidates, totalling 400 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy_score(best_estimator_sgd.predict(x_test) , y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kS-rOL0bNhrn",
        "outputId": "b1f7ce3a-b553-4a1c-de77-914219e59fc7"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5654450261780105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_estimator_sgd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "yQ5RuxqqOS-I",
        "outputId": "b4c68911-939b-4b2b-f935-84ed0fb96512"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(alpha=1e-05, early_stopping=True, eta0=0.001, tol=0.0001)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(alpha=1e-05, early_stopping=True, eta0=0.001, tol=0.0001)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(alpha=1e-05, early_stopping=True, eta0=0.001, tol=0.0001)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_search.best_params_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-TRu5YYNwhx",
        "outputId": "0c9e1832-9163-4a58-bed8-56dddeb03404"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'tol': 0.0001,\n",
              " 'max_iter': 1000,\n",
              " 'loss': 'hinge',\n",
              " 'learning_rate': 'optimal',\n",
              " 'eta0': 0.001,\n",
              " 'alpha': 1e-05}"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gridserach cv for the SGD\n",
        "alpha = [0.000001,0.000005,0.000007,.00001 ,0.00003 , 0.0005 , 0.0007]\n",
        "tol = [0.00001, 0.00005 , 0.0001 , 0.0003]\n",
        "max_iter = [700 , 1000 ,1300 , 1600]\n",
        "eta0 = [0.0001,0.0005,0.0007,0.001 , 0.005 , 0.003]\n",
        "param_grid = {\n",
        "    'alpha' : alpha,\n",
        "    'tol' : tol,\n",
        "    'max_iter': max_iter,\n",
        "    'eta0' : eta0\n",
        "}\n",
        "sgd = SGDClassifier(early_stopping=True)\n",
        "grid_search = GridSearchCV(estimators = sgd , scoring = 'accuracy' , param_grid = param_grid , n_jobs = -1 , cv = 4,verbose = 2 , error_score='raise' )\n",
        "grid_search.fit(x_train,y_train)\n",
        "sgd_grid_best_estimator = grid_search.best_estimators"
      ],
      "metadata": {
        "id": "wUIZ0W7BOxxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDLSD9qt__kW"
      },
      "outputs": [],
      "source": [
        "# Naive bayes\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(x_train,y_train)\n",
        "# gnb.partial_fit(x_train,y_train,[0,1])\n",
        "gnb.score(x_test,y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZJVW6s2COZa"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "mun = MultinomialNB(alpha = 1 , force_alpha='warn')\n",
        "mun.fit(x_train,y_train)\n",
        "mun.score(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZYPrqPoQt-a"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import ComplementNB\n",
        "cmb = ComplementNB(alpha = 1 , force_alpha = 'warn')\n",
        "cmb.fit(x_train,y_train)\n",
        "cmb.score(x_test,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ju-ymMMu3SoC"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "estimators  = [('gaussain',gnb),('Multinomial',mun),('Complement',cmb)]\n",
        "VCL = VotingClassifier(estimators = estimators)\n",
        "voting = ['hard','soft']\n",
        "param_grid  = {\n",
        "    'voting' : voting\n",
        "}\n",
        "grid_search = GridSearchCV(estimator = VCL , scoring = 'accuracy' , param_grid = param_grid , cv = 5 , n_jobs = -1 , verbose = 2)\n",
        "grid_search.fit(x_train,y_train)\n",
        "best_param_VCl = grid_search.best_estimator_\n",
        "print(accuracy_score( best_param_VCl.predict(x_test), y_test ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7auBNFUV7YOb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXR8cyIT_-0K"
      },
      "outputs": [],
      "source": [
        "# xcgvxcv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9krJ4mwT0sQ"
      },
      "outputs": [],
      "source": [
        "# C = [0.05 , 0.1 , 0.5  , 1 , 5 , 10 ,15 , 25]\n",
        "# kernel = ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "# gamma = ['scale','auto']\n",
        "# degree = [int(x) for x in range(1,2)]\n",
        "# param_grid = {\n",
        "#     'kernel' : kernel,\n",
        "#     'gamma' : gamma,\n",
        "#     'degree' : degree,\n",
        "#     'C' : C\n",
        "# }\n",
        "# svc = SVC()\n",
        "# grid_search = GridSearchCV(estimator=svc,scoring='accuracy',param_grid=param_grid,cv = 3,n_jobs = -1,verbose = 6)\n",
        "# grid_search.fit(x_train,y_train)\n",
        "# best_gird_search = grid_search.best_estimator_\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# y_pred = best_gird_search.predict(x_test)\n",
        "# print(confusion_matrix(y_test,y_pred))\n",
        "# print(\"Accuracy Score {}\".format(accuracy_score(y_test,y_pred)))\n",
        "# print(accuracy_score(y_train,best_gird_search.predict(x_train)))\n",
        "# Score = cross_val_score(best_gird_search,X_data,Y_data,cv = 5)\n",
        "# Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o66bh0haFdSJ"
      },
      "outputs": [],
      "source": [
        "# grid_search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbhpT4dET0sR"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics import accuracy_score\n",
        "# model = grid_search.best_params_\n",
        "# model.fit(x_train,y_train)\n",
        "# y_pred = model.predict(x_test)\n",
        "# print(confusion_matrix(y_test,y_pred))\n",
        "# print(\"Accuracy for the trained data is {}\".format(accuracy_score(y_train,model.predict(x_train))))\n",
        "# print(\"Accuracy Score {}\".format(accuracy_score(y_test,y_pred)))\n",
        "# print(\"classification report {}\".format(classification_report(y_test,y_pred)))\n",
        "# Score = cross_val_score(model,X_data,Y_data,cv = 5)\n",
        "# Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JasQiE2XXIZX"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "373C8LooT0sR"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dense\n",
        "# model = Sequential()\n",
        "# model.add(Dense(12,input_shape = (x_train.shape[1],),activation = 'relu'))\n",
        "# model.add(Dense(8,activation = 'relu'))\n",
        "# model.add(Dense(1,activation = 'sigmoid'))\n",
        "# model.compile(loss = 'binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "# model.fit(x_train,y_train,epochs=200,batch_size=1000)\n",
        "# predictions = (model.predict(x_test) > 0.5).astype(int)\n",
        "# abs(predictions.T - y_test).sum()/y_test.shape[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWFCEGKzT0sR"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "score_test = []\n",
        "score_train = []\n",
        "scale = [i for i in range(1,200,5)]\n",
        "for i in range(1 , 200 , 5):\n",
        "  randomclassifier=RandomForestClassifier(n_estimators=i,criterion='entropy')\n",
        "  randomclassifier.fit(x_train,y_train)\n",
        "  predictions = randomclassifier.predict(x_test)\n",
        "  predictions_train = randomclassifier.predict(x_train)\n",
        "  score_test.append(accuracy_score(y_test,predictions))\n",
        "  score_train.append(accuracy_score(y_train,predictions_train))\n",
        "plt.plot(scale ,score_test )\n",
        "plt.plot(scale ,score_train )\n",
        "  # matrix=confusion_matrix(y_test,predictions)\n",
        "# print(matrix)\n",
        "# score=\n",
        "# print(score)\n",
        "# report=classification_report(y_test,predictions)\n",
        "# print(report)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}