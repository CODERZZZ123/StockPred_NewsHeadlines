{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import numpy as np # linear algebra\n",
    "import os # accessing directory structure\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 rows and 27 columns\n"
     ]
    }
   ],
   "source": [
    "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
    "df1 = pd.read_csv('Combined_News_DJIA.csv', delimiter=',', nrows = nRowsRead)\n",
    "df1.dataframeName = 'Combined_News_DJIA.csv'\n",
    "nRow, nCol = df1.shape\n",
    "print(f'There are {nRow} rows and {nCol} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 rows and 7 columns\n"
     ]
    }
   ],
   "source": [
    "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
    "# DJIA_table.csv has 1989 rows in reality, but we are only loading/previewing the first 1000 rows\n",
    "df2 = pd.read_csv('upload_DJIA_table.csv', delimiter=',', nrows = nRowsRead)\n",
    "df2.dataframeName = 'upload_DJIA_table.csv'\n",
    "nRow, nCol = df2.shape\n",
    "print(f'There are {nRow} rows and {nCol} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 rows and 2 columns\n"
     ]
    }
   ],
   "source": [
    "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
    "# RedditNews.csv has 73608 rows in reality, but we are only loading/previewing the first 1000 rows\n",
    "df3 = pd.read_csv('RedditNews.csv', delimiter=',', nrows = nRowsRead)\n",
    "df3.dataframeName = 'RedditNews.csv'\n",
    "nRow, nCol = df3.shape\n",
    "print(f'There are {nRow} rows and {nCol} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\999sb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenize\u001b[39;00m \u001b[39mimport\u001b[39;00m word_tokenize\n\u001b[0;32m      6\u001b[0m \u001b[39m# create a list of columns containing text\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m text_columns \u001b[39m=\u001b[39m [col \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39mcolumns \u001b[39mif\u001b[39;00m col\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mTop\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[0;32m      9\u001b[0m \u001b[39m# tokenize the text in all columns and store the result in a new column with the same name + \"_tokenized\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m text_columns:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# create a list of columns containing text\n",
    "text_columns = [col for col in df.columns if col.startswith('Top')]\n",
    "\n",
    "# tokenize the text in all columns and store the result in a new column with the same name + \"_tokenized\"\n",
    "for col in text_columns:\n",
    "    df[col + '_tokenized'] = df[col].apply(word_tokenize)\n",
    "\n",
    "# show the first 5 rows of the new columns\n",
    "print(df[[col + '_tokenized' for col in text_columns]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[text_columns] = df[text_columns].apply(lambda x: x.str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "df[text_columns] = df[text_columns].apply(lambda x: x.apply(lemmatize_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download the stopwords and punkt corpora from nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # Create a set of English stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # Tokenize the input text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Convert words to lowercase and remove stop words\n",
    "    words = [word.lower() for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # Join the words back into a single string\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "# Loop through all columns in the dataframe\n",
    "for col in df.columns:\n",
    "    # Check if the column data type is object\n",
    "    if df[col].dtype == 'object':\n",
    "        # Apply the remove_stopwords function to the column if the value is a string\n",
    "        df[col] = df[col].apply(lambda x: remove_stopwords(x) if type(x) == str else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Define a function to remove punctuation from a string\n",
    "def remove_punctuation(text):\n",
    "    # Create a string of all ASCII punctuation characters\n",
    "    punctuation = string.punctuation\n",
    "    # Use translate method to remove all punctuation characters from the text\n",
    "    text = text.translate(str.maketrans(\"\", \"\", punctuation))\n",
    "    return text\n",
    "\n",
    "# Loop through all columns in the dataframe\n",
    "for col in df.columns:\n",
    "    # Check if the column data type is object (e.g. string)\n",
    "    if df[col].dtype == 'object':\n",
    "        # Apply the remove_punctuation function to each string value in the column\n",
    "        df[col] = df[col].apply(lambda x: remove_punctuation(x) if type(x) == str else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_non_textual(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # remove URLs\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # remove numbers\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # remove special characters\n",
    "    return text\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col] = df[col].apply(lambda x: remove_non_textual(x) if type(x) == str else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "def remove_corpus_specific_words(text, threshold):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    fdist = FreqDist(words)\n",
    "    stop_words = [word for word, count in fdist.items() if count > threshold]\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col] = df[col].apply(lambda x: remove_corpus_specific_words(x, 100) if type(x) == str else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Label','Top1_tokenized', 'Top2_tokenized', 'Top3_tokenized',           'Top4_tokenized', 'Top5_tokenized', 'Top6_tokenized', 'Top7_tokenized',           'Top8_tokenized', 'Top9_tokenized', 'Top10_tokenized',           'Top11_tokenized', 'Top12_tokenized', 'Top13_tokenized',           'Top14_tokenized', 'Top15_tokenized', 'Top16_tokenized',           'Top17_tokenized', 'Top18_tokenized', 'Top19_tokenized',           'Top20_tokenized', 'Top21_tokenized', 'Top22_tokenized',           'Top23_tokenized', 'Top24_tokenized', 'Top25_tokenized']\n",
    "df = df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf_features(dataframe):\n",
    "    tfidf = TfidfVectorizer(min_df=2)  # minimum document frequency to consider a word\n",
    "    features = tfidf.fit_transform(dataframe)\n",
    "    feature_names = tfidf.get_feature_names()\n",
    "    return features, feature_names\n",
    "\n",
    "# Apply TF-IDF to all columns with text data\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        text = df[col].apply(str)  # convert pandas series to list of strings\n",
    "        features, feature_names = tfidf_features(text)\n",
    "        df_tfidf = pd.DataFrame(features.todense(), columns=feature_names)\n",
    "        df = pd.concat([df, df_tfidf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "\n",
    "# extract the text data\n",
    "text_data = df[['Top1_tokenized', 'Top2_tokenized', 'Top3_tokenized',           'Top4_tokenized', 'Top5_tokenized', 'Top6_tokenized', 'Top7_tokenized',           'Top8_tokenized', 'Top9_tokenized', 'Top10_tokenized',           'Top11_tokenized', 'Top12_tokenized', 'Top13_tokenized',           'Top14_tokenized', 'Top15_tokenized', 'Top16_tokenized',           'Top17_tokenized', 'Top18_tokenized', 'Top19_tokenized',           'Top20_tokenized', 'Top21_tokenized', 'Top22_tokenized',           'Top23_tokenized', 'Top24_tokenized', 'Top25_tokenized']]\n",
    "\n",
    "# combine all the text data into a single string for each row\n",
    "text_data = text_data.apply(lambda x: \" \".join(str(x) for x in x), axis=1)\n",
    "\n",
    "# perform count vectorization to convert text into numerical data\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(text_data)\n",
    "\n",
    "# extract the target variable\n",
    "y = df[\"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "# Define the models to be trained\n",
    "models = [LogisticRegression(), SVC(), RandomForestClassifier(), GradientBoostingClassifier(), KNeighborsClassifier(), DecisionTreeClassifier()]\n",
    "model_names = [\"Logistic Regression\", \"Support Vector Classifier\", \"Random Forest\", \"Gradient Boosting\", \"K-Nearest Neighbors\", \"Decision Tree\"]\n",
    "\n",
    "results=[]\n",
    "# Train each model and get predictions\n",
    "for model, model_name in zip(models, model_names):\n",
    "    model.fit(train_data, train_labels)\n",
    "    predictions = model.predict(test_data)\n",
    "    \n",
    "    # Calculate evaluation metrics for each model\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    precision = precision_score(test_labels, predictions)\n",
    "    recall = recall_score(test_labels, predictions)\n",
    "    f1 = f1_score(test_labels, predictions)\n",
    "    \n",
    "    # Store the results of each model in the results list\n",
    "    results.append((model_name, accuracy, precision, recall, f1))\n",
    "    \n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(\"Model: {}\\nAccuracy: {:.2f}%\\nPrecision: {:.2f}%\\nRecall: {:.2f}%\\nF1 Score: {:.2f}%\\n\".format(result[0], result[1]*100, result[2]*100, result[3]*100, result[4]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
